{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "from kafka import KafkaProducer\n",
    "from schema import Schema, SchemaError\n",
    "log = logging.getLogger()\n",
    "logging.basicConfig(level=os.environ.get(\"LOGLEVEL\", \"INFO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value(key):\n",
    "    match key:\n",
    "        case 'technology':\n",
    "            print(key)\n",
    "            \n",
    "        case 'hostedAt':\n",
    "            print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import Schema, SchemaError, Optional, Hook, Or\n",
    "\n",
    "schema_val = {\n",
    "    \"name\": str,\n",
    "    \"description\": str,\n",
    "    \"status\": str,\n",
    "\n",
    "    \"consumers\": {\n",
    "        \"name\": str,\n",
    "        \"description\": str,\n",
    "        \"type\" : str\n",
    "    },\n",
    "    \"containers\": {\n",
    "        \"name\": str,\n",
    "        \"sysnonyms\": str,\n",
    "        \"description\": str,\n",
    "        Optional(\"technology\", default= lambda : add_value('technology')): str,\n",
    "        \"parentSystem\": str,\n",
    "        \"ciDataOwner\": str,\n",
    "        \"productOwner\": str,\n",
    "        \"applicationType\": Or(\"Business\", \"Customer Facing\", \"External Service\", \"Infrastructure\", \"Interface\", \"Office\", \"Tool\", \"Unknown\"),\n",
    "        Optional(\"hostedAt\", default = lambda : add_value('hostedAt')): Or(\"Amazon Web Services (AWS Cloud)\", \"AT&T\", \"Azure CF1\", \"Azure CF2\", \"Azure Cloud\", \"DXC\", \"Equinix\", \"Google Cloud Platform\", \"Hybric\", \"Inlumi\", \"Local server\", \"Multi-Cloud\", \"Not Applicable\", \"Other\", \"Salesforce\", \"ServiceNow\", \"Solvinity\", \"Unit4\", \"Unknown\", \"User device\", \"Azure\"),\n",
    "        \"deploymentModel\": Or(\"BPO\", \"CaaS\", \"IaaS\", \"On-Premise\", \"PaaS\", \"SaaS\"),\n",
    "        \"personalData\": bool,\n",
    "        \"confidentiality\": str,\n",
    "        \"mcv\": Or(\"Highly business critical\", \"Business critical\", \"Not business critical\", \"Not applicable\"),\n",
    "        \"maxSeverityLevel\": Or(1,2,3,4, \"Not applicable\"),\n",
    "        Optional(\"sox\", default= lambda : add_value('sox')): bool,\n",
    "        Optional(\"icfr\", default= lambda : add_value('icfr')): bool,\n",
    "        \"assignementGroup\": str,\n",
    "        \"operationalStatus\": Or(\"Pipelined\", \"Operational\", \"Non-Operational\", \"Submitted for decommissioning\", \"Decommissioned\", \"In decommissioning process\"),\n",
    "        \"environments\": Or(\"nl\", \"be\"),\n",
    "        \"relationships\": {\n",
    "            \"type\": str,\n",
    "            \"container\": {\n",
    "                \"name\": str,\n",
    "            },\n",
    "        },\n",
    "        \"components\": {\n",
    "            \"name\": str,\n",
    "            \"description\": str,\n",
    "            \"exposedAPIs\": {\n",
    "                \"name\": str,\n",
    "                \"description\": str,\n",
    "                \"type\": str,\n",
    "                \"status\": str,\n",
    "            },\n",
    "            \"consumedAPIs\": {\n",
    "                \"name\": str,\n",
    "                \"description\": str,\n",
    "                \"status\": str\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_yaml(yaml_data):\n",
    "    #schema = eval(open('./schema.yml', 'r').read())\n",
    "    validator = Schema(schema_val)\n",
    "    try:\n",
    "        validator.validate(yaml_data)\n",
    "        print('YML valid')\n",
    "    except SchemaError as se:\n",
    "        print(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc():\n",
    "    with open('./test.yml', 'r', encoding='utf8') as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'poc-git-to-cmdb', 'description': 'POC to send information about the app to Kafka', 'status': 'pipelined', 'consumers': {'name': 'Developer', 'description': 'A developer who keeps properties file up to date', 'type': 'PERSON'}, 'containers': {'name': 'poc-git-tocmdb', 'sysnonyms': 'poc-git-to-kafka-cmdb-sync', 'description': 'POC', 'technology': 'Kafka', 'parentSystem': 'CMDB', 'ciDataOwner': 'Aede van der Weij', 'productOwner': 'Thomas de Vries', 'applicationType': 'Tool', 'hostedAt': 'Azure', 'deploymentModel': 'On-Premise', 'personalData': False, 'confidentiality': 'Internal use', 'mcv': 'Not business critical', 'maxSeverityLevel': 4, 'sox': False, 'icfr': False, 'assignementGroup': 'AMS_ITOnline_L3_SRE_Infra', 'operationalStatus': 'Pipelined', 'environments': 'nl', 'relationships': {'type': 'relBack', 'container': {'name': 'Container Name'}}, 'components': {'name': 'Component name', 'description': 'what the system does', 'exposedAPIs': {'name': 'Unique API name', 'description': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}, 'consumedAPIs': {'name': 'reference to API name', 'description': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED'}}}}\n"
     ]
    }
   ],
   "source": [
    "import avro.schema\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter\n",
    "avro_schema = avro.schema.parse(open(\"avro_schema.avsc\", \"rb\").read())\n",
    "\n",
    "writer = DataFileWriter(open(\"users.avro\", \"wb\"), DatumWriter(), avro_schema)\n",
    "writer.append(load_doc())\n",
    "writer.close()\n",
    "\n",
    "reader = DataFileReader(open(\"users.avro\", \"rb\"), DatumReader())\n",
    "\n",
    "for item in reader:\n",
    "    print(item)\n",
    "\n",
    "os.remove(\"users.avro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YML valid\n"
     ]
    }
   ],
   "source": [
    "doc = load_doc()\n",
    "validate_yaml(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "  \"type\" : \"record\",\n",
    "  \"namespace\" : \"com.test.avro\",\n",
    "  \"name\" : \"SystemModel\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"name\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"description\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"status\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"consumers\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"consumers\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"name\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"description\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"type\",\n",
    "        \"type\" : \"string\"\n",
    "      } ]\n",
    "    }\n",
    "  }, {\n",
    "    \"name\" : \"containers\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"containers\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"name\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"sysnonyms\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"description\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"technology\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"parentSystem\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"ciDataOwner\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"productOwner\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"applicationType\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"hostedAt\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"deploymentModel\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"personalData\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"confidentiality\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"mcv\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"maxSeverityLevel\",\n",
    "        \"type\" : \"long\"\n",
    "      }, {\n",
    "        \"name\" : \"sox\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"icfr\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"assignementGroup\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"operationalStatus\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"environments\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"relationships\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"record\",\n",
    "          \"name\" : \"relationships\",\n",
    "          \"fields\" : [ {\n",
    "            \"name\" : \"type\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"container\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"container\",\n",
    "              \"fields\" : [ {\n",
    "                \"name\" : \"name\",\n",
    "                \"type\" : \"string\"\n",
    "              } ]\n",
    "            }\n",
    "          } ]\n",
    "        }\n",
    "      }, {\n",
    "        \"name\" : \"components\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"record\",\n",
    "          \"name\" : \"components\",\n",
    "          \"fields\" : [ {\n",
    "            \"name\" : \"name\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"description\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"exposedAPIs\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"exposedAPIs\",\n",
    "              \"fields\" : [ { \"name\" : \"name\", \"type\" : \"string\" }, \n",
    "              { \"name\" : \"description\", \"type\" : \"string\" }, \n",
    "              { \"name\" : \"type\", \"type\" : \"string\" }, \n",
    "              { \"name\" : \"status\", \"type\" : \"string\" } ]\n",
    "            }\n",
    "          }, {\n",
    "            \"name\" : \"consumedAPIs\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"consumedAPIs\",\n",
    "              \"fields\" : [ {\n",
    "                \"name\" : \"name\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"description\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"status\",\n",
    "                \"type\" : \"string\"\n",
    "              } ]\n",
    "            }\n",
    "          } ]\n",
    "        }\n",
    "      } ]\n",
    "    }\n",
    "  } \n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = \"\"\"\n",
    "{\n",
    "  \"type\" : \"record\",\n",
    "  \"namespace\" : \"com.test.avro\",\n",
    "  \"name\" : \"SystemModel\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"name\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"description\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"status\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"consumers\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"consumers\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"name\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"description\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"type\",\n",
    "        \"type\" : \"string\"\n",
    "      } ]\n",
    "    }\n",
    "  }, {\n",
    "    \"name\" : \"containers\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"containers\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"name\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"sysnonyms\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"description\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"technology\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"parentSystem\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"ciDataOwner\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"productOwner\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"applicationType\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"hostedAt\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"deploymentModel\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"personalData\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"confidentiality\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"mcv\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"maxSeverityLevel\",\n",
    "        \"type\" : \"long\"\n",
    "      }, {\n",
    "        \"name\" : \"sox\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"icfr\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"assignementGroup\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"operationalStatus\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"environments\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"relationships\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"record\",\n",
    "          \"name\" : \"relationships\",\n",
    "          \"fields\" : [ {\n",
    "            \"name\" : \"type\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"container\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"container\",\n",
    "              \"fields\" : [ {\n",
    "                \"name\" : \"name\",\n",
    "                \"type\" : \"string\"\n",
    "              } ]\n",
    "            }\n",
    "          } ]\n",
    "        }\n",
    "      }, {\n",
    "        \"name\" : \"components\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"record\",\n",
    "          \"name\" : \"components\",\n",
    "          \"fields\" : [ {\n",
    "            \"name\" : \"name\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"description\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"exposedAPIs\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"exposedAPIs\",\n",
    "              \"fields\" : [ { \"name\" : \"name\", \"type\" : \"string\" }, \n",
    "              { \"name\" : \"description\", \"type\" : \"string\" }, \n",
    "              { \"name\" : \"type\", \"type\" : \"string\" }, \n",
    "              { \"name\" : \"status\", \"type\" : \"string\" } ]\n",
    "            }\n",
    "          }, {\n",
    "            \"name\" : \"consumedAPIs\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"consumedAPIs\",\n",
    "              \"fields\" : [ {\n",
    "                \"name\" : \"name\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"description\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"status\",\n",
    "                \"type\" : \"string\"\n",
    "              } ]\n",
    "            }\n",
    "          } ]\n",
    "        }\n",
    "      } ]\n",
    "    }\n",
    "  } \n",
    "  ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_key_str = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"TestObject\",\n",
    "    \"namespace\": \"System-key\",\n",
    "    \"fields\": [{\n",
    "        \"name\": \"key\",\n",
    "        \"type\": \"string\"\n",
    "    }]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1667317266.144|TERMINATE|rdkafka#producer-24| [thrd:app]: Producer terminating with 1 message (522 bytes) still in queue or transit: use flush() to wait for outstanding message delivery\n"
     ]
    }
   ],
   "source": [
    "topic = \"topic5\"\n",
    "\n",
    "with open(\"avro_schema.avsc\") as f:\n",
    "    schema_str = f.read()\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient({'url': 'http://10.152.183.242:8081'})\n",
    "\n",
    "avro_serializer = AvroSerializer(schema_registry_client, schema_str)\n",
    "\n",
    "string_serializer = StringSerializer('utf_8')\n",
    "\n",
    "producer = Producer({'bootstrap.servers': '10.152.183.181:9094'})\n",
    "\n",
    "producer.produce(topic=topic, key=string_serializer('testkey', None), value=avro_serializer(data, SerializationContext(topic, MessageField.VALUE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [59], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconfluent_kafka\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema_registry\u001b[39;00m \u001b[39mimport\u001b[39;00m SchemaRegistryClient, Schema\n\u001b[1;32m      3\u001b[0m avro_schema \u001b[39m=\u001b[39m Schema(schema_str, \u001b[39m'\u001b[39m\u001b[39mAVRO\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m client \u001b[39m=\u001b[39m SchemaRegistryClient(\u001b[39m\"\u001b[39;49m\u001b[39mhttp://10.152.183.242:8081\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m schema_id \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mregister_schema(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, avro_schema)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/confluent_kafka/schema_registry/schema_registry_client.py:292\u001b[0m, in \u001b[0;36mSchemaRegistryClient.__init__\u001b[0;34m(self, conf)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, conf):\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rest_client \u001b[39m=\u001b[39m _RestClient(conf)\n\u001b[1;32m    293\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache \u001b[39m=\u001b[39m _SchemaCache()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/confluent_kafka/schema_registry/schema_registry_client.py:66\u001b[0m, in \u001b[0;36m_RestClient.__init__\u001b[0;34m(self, conf)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m Session()\n\u001b[1;32m     65\u001b[0m \u001b[39m# copy dict to avoid mutating the original\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m conf_copy \u001b[39m=\u001b[39m conf\u001b[39m.\u001b[39;49mcopy()\n\u001b[1;32m     68\u001b[0m base_url \u001b[39m=\u001b[39m conf_copy\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m base_url \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient, Schema\n",
    "\n",
    "avro_schema = Schema(schema_str, 'AVRO')\n",
    "\n",
    "client = SchemaRegistryClient(\"http://10.152.183.242:8081\")\n",
    "\n",
    "schema_id = client.register_schema('test', avro_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32mfastavro/_write.pyx:363\u001b[0m, in \u001b[0;36mfastavro._write.write_record\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected dict, got str",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconfluent_kafka\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mavro\u001b[39;00m \u001b[39mimport\u001b[39;00m AvroProducer\n\u001b[1;32m      3\u001b[0m producer \u001b[39m=\u001b[39m AvroProducer({\u001b[39m'\u001b[39m\u001b[39mbootstrap.servers\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m10.152.183.181:9094\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mschema.registry.url\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mhttp://10.152.183.242:8081\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m----> 5\u001b[0m producer\u001b[39m.\u001b[39;49mproduce(topic\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtopic4\u001b[39;49m\u001b[39m\"\u001b[39;49m, value\u001b[39m=\u001b[39;49mdata, value_schema\u001b[39m=\u001b[39;49mschema_str, key_schema\u001b[39m=\u001b[39;49mschema_key_str, key\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtestkey\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/confluent_kafka/avro/__init__.py:105\u001b[0m, in \u001b[0;36mAvroProducer.produce\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m key_schema:\n\u001b[0;32m--> 105\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_serializer\u001b[39m.\u001b[39;49mencode_record_with_schema(topic, key_schema, key, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    106\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m         \u001b[39mraise\u001b[39;00m KeySerializerError(\u001b[39m\"\u001b[39m\u001b[39mAvro schema required for key\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/confluent_kafka/avro/serializer/message_serializer.py:120\u001b[0m, in \u001b[0;36mMessageSerializer.encode_record_with_schema\u001b[0;34m(self, topic, schema, record, is_key)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m schema_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_to_writers:\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_to_writers[schema_id] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_encoder_func(schema)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_record_with_schema_id(schema_id, record, is_key\u001b[39m=\u001b[39;49mis_key)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/confluent_kafka/avro/serializer/message_serializer.py:154\u001b[0m, in \u001b[0;36mMessageSerializer.encode_record_with_schema_id\u001b[0;34m(self, schema_id, record, is_key)\u001b[0m\n\u001b[1;32m    151\u001b[0m outf\u001b[39m.\u001b[39mwrite(struct\u001b[39m.\u001b[39mpack(\u001b[39m'\u001b[39m\u001b[39m>bI\u001b[39m\u001b[39m'\u001b[39m, MAGIC_BYTE, schema_id))\n\u001b[1;32m    153\u001b[0m \u001b[39m# write the record to the rest of the buffer\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m writer(record, outf)\n\u001b[1;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m outf\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/confluent_kafka/avro/serializer/message_serializer.py:85\u001b[0m, in \u001b[0;36mMessageSerializer._get_encoder_func.<locals>.<lambda>\u001b[0;34m(record, fp)\u001b[0m\n\u001b[1;32m     83\u001b[0m     schema \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(\u001b[39mstr\u001b[39m(writer_schema))\n\u001b[1;32m     84\u001b[0m     parsed_schema \u001b[39m=\u001b[39m parse_schema(schema)\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m record, fp: schemaless_writer(fp, parsed_schema, record)\n\u001b[1;32m     86\u001b[0m writer \u001b[39m=\u001b[39m avro\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mDatumWriter(writer_schema)\n\u001b[1;32m     87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m record, fp: writer\u001b[39m.\u001b[39mwrite(record, avro\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mBinaryEncoder(fp))\n",
      "File \u001b[0;32mfastavro/_write.pyx:786\u001b[0m, in \u001b[0;36mfastavro._write.schemaless_writer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfastavro/_write.pyx:450\u001b[0m, in \u001b[0;36mfastavro._write.write_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfastavro/_write.pyx:378\u001b[0m, in \u001b[0;36mfastavro._write.write_record\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.avro import AvroProducer\n",
    "\n",
    "producer = AvroProducer({'bootstrap.servers': '10.152.183.181:9094', 'schema.registry.url': 'http://10.152.183.242:8081'})\n",
    "\n",
    "producer.produce(topic=\"topic4\", value=data, value_schema=schema_str, key_schema=schema_key_str, key=\"testkey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connecting> [IPv4 ('10.152.183.181', 9094)]>: connecting to 10.152.183.181:9094 [('10.152.183.181', 9094) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connecting> [IPv4 ('10.152.183.181', 9094)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=192.168.0.121:30000 <connected> [IPv4 ('192.168.0.121', 30000)]>: Closing connection. \n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=192.168.0.121:30000 <connecting> [IPv4 ('192.168.0.121', 30000)]>: connecting to 192.168.0.121:30000 [('192.168.0.121', 30000) IPv4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7fef62ebbac0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=192.168.0.121:30000 <connecting> [IPv4 ('192.168.0.121', 30000)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connected> [IPv4 ('10.152.183.181', 9094)]>: Closing connection. \n"
     ]
    }
   ],
   "source": [
    "producer = KafkaProducer(\n",
    "                             value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                             bootstrap_servers=\"10.152.183.181:9094\")\n",
    "producer.send('topic2', value=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connecting> [IPv4 ('10.152.183.181', 9094)]>: connecting to 10.152.183.181:9094 [('10.152.183.181', 9094) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connecting> [IPv4 ('10.152.183.181', 9094)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=192.168.0.121:30000 <connecting> [IPv4 ('192.168.0.121', 30000)]>: connecting to 192.168.0.121:30000 [('192.168.0.121', 30000) IPv4]\n",
      "INFO:kafka.conn:Probing node 0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=192.168.0.121:30000 <connecting> [IPv4 ('192.168.0.121', 30000)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connected> [IPv4 ('10.152.183.181', 9094)]>: Closing connection. \n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka_schema_registry:Not recreating existing topic topic1\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connecting> [IPv4 ('10.152.183.181', 9094)]>: connecting to 10.152.183.181:9094 [('10.152.183.181', 9094) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connecting> [IPv4 ('10.152.183.181', 9094)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connected> [IPv4 ('10.152.183.181', 9094)]>: Closing connection. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7f1d80d845b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=192.168.0.121:30000 <connecting> [IPv4 ('192.168.0.121', 30000)]>: connecting to 192.168.0.121:30000 [('192.168.0.121', 30000) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=192.168.0.121:30000 <connecting> [IPv4 ('192.168.0.121', 30000)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connected> [IPv4 ('10.152.183.181', 9094)]>: Closing connection. \n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=192.168.0.121:30000 <connecting> [IPv4 ('192.168.0.121', 30000)]>: connecting to 192.168.0.121:30000 [('192.168.0.121', 30000) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=192.168.0.121:30000 <connecting> [IPv4 ('192.168.0.121', 30000)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.152.183.181:9094 <connected> [IPv4 ('10.152.183.181', 9094)]>: Closing connection. \n"
     ]
    }
   ],
   "source": [
    "from kafka_schema_registry import prepare_producer\n",
    "\n",
    "producer = prepare_producer(bootstrap_servers=[\"10.152.183.181:9094\"], avro_schema_registry=\"http://10.152.183.242:8081\", topic_name=\"topic1\", value_schema=schema, num_partitions=1, replication_factor=1)\n",
    "\n",
    "producer.send(\"topic1\",data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
