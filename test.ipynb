{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "from kafka import KafkaProducer\n",
    "from schema import Schema, SchemaError\n",
    "log = logging.getLogger()\n",
    "logging.basicConfig(level=os.environ.get(\"LOGLEVEL\", \"INFO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value(key):\n",
    "    match key:\n",
    "        case 'technology':\n",
    "            print(key)\n",
    "            \n",
    "        case 'hostedAt':\n",
    "            print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_yaml(yaml_file: str) -> dict:\n",
    "    with open(yaml_file, mode='r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "def find_main_language(full_output = False):\n",
    "  print('language')\n",
    "  matches = []\n",
    "  for root, directory, filenames in os.walk(os.getcwd()):\n",
    "      for filename in filenames:\n",
    "        if re.search(\".(\\b.py\\b)\", filename):\n",
    "          print(filename)\n",
    "          print(os.path.getsize(root + '/' + filename))\n",
    "  print(matches)\n",
    "\n",
    "find_main_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import Schema, SchemaError, Optional, Hook, Or\n",
    "\n",
    "schema_val = {\n",
    "    \"name\": str,\n",
    "    \"description\": str,\n",
    "    \"status\": str,\n",
    "\n",
    "    \"consumers\": {\n",
    "        \"name\": str,\n",
    "        \"description\": str,\n",
    "        \"type\" : str\n",
    "    },\n",
    "    \"containers\": {\n",
    "        \"name\": str,\n",
    "        \"sysnonyms\": str,\n",
    "        \"description\": str,\n",
    "        Optional(\"technology\", default= lambda : add_value('technology')): str,\n",
    "        \"parentSystem\": str,\n",
    "        \"ciDataOwner\": str,\n",
    "        \"productOwner\": str,\n",
    "        \"applicationType\": Or(\"Business\", \"Customer Facing\", \"External Service\", \"Infrastructure\", \"Interface\", \"Office\", \"Tool\", \"Unknown\"),\n",
    "        Optional(\"hostedAt\", default = lambda : add_value('hostedAt')): Or(\"Amazon Web Services (AWS Cloud)\", \"AT&T\", \"Azure CF1\", \"Azure CF2\", \"Azure Cloud\", \"DXC\", \"Equinix\", \"Google Cloud Platform\", \"Hybric\", \"Inlumi\", \"Local server\", \"Multi-Cloud\", \"Not Applicable\", \"Other\", \"Salesforce\", \"ServiceNow\", \"Solvinity\", \"Unit4\", \"Unknown\", \"User device\", \"Azure\"),\n",
    "        \"deploymentModel\": Or(\"BPO\", \"CaaS\", \"IaaS\", \"On-Premise\", \"PaaS\", \"SaaS\"),\n",
    "        \"personalData\": bool,\n",
    "        \"confidentiality\": str,\n",
    "        \"mcv\": Or(\"Highly business critical\", \"Business critical\", \"Not business critical\", \"Not applicable\"),\n",
    "        \"maxSeverityLevel\": Or(1,2,3,4, \"Not applicable\"),\n",
    "        Optional(\"sox\", default= lambda : add_value('sox')): bool,\n",
    "        Optional(\"icfr\", default= lambda : add_value('icfr')): bool,\n",
    "        \"assignementGroup\": str,\n",
    "        \"operationalStatus\": Or(\"Pipelined\", \"Operational\", \"Non-Operational\", \"Submitted for decommissioning\", \"Decommissioned\", \"In decommissioning process\"),\n",
    "        \"environments\": Or(\"nl\", \"be\"),\n",
    "        \"relationships\": {\n",
    "            \"type\": str,\n",
    "            \"container\": {\n",
    "                \"name\": str,\n",
    "            },\n",
    "        },\n",
    "        \"components\": {\n",
    "            \"name\": str,\n",
    "            \"description\": str,\n",
    "            \"exposedAPIs\": {\n",
    "                \"name\": str,\n",
    "                \"description\": str,\n",
    "                \"type\": str,\n",
    "                \"status\": str,\n",
    "            },\n",
    "            \"consumedAPIs\": {\n",
    "                \"name\": str,\n",
    "                \"description\": str,\n",
    "                \"status\": str\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_yaml(yaml_data):\n",
    "    #schema = eval(open('./schema.yml', 'r').read())\n",
    "    validator = Schema(schema_val)\n",
    "    try:\n",
    "        validator.validate(yaml_data)\n",
    "        print('YML valid')\n",
    "    except SchemaError as se:\n",
    "        print(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc():\n",
    "    with open('./test.yml', 'r', encoding='utf8') as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avro.schema\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter\n",
    "avro_schema = avro.schema.parse(open(\"avro_schema.avsc\", \"rb\").read())\n",
    "\n",
    "writer = DataFileWriter(open(\"users.avro\", \"wb\"), DatumWriter(), avro_schema)\n",
    "writer.append(load_doc())\n",
    "writer.close()\n",
    "\n",
    "reader = DataFileReader(open(\"users.avro\", \"rb\"), DatumReader())\n",
    "\n",
    "for item in reader:\n",
    "    print(item)\n",
    "\n",
    "os.remove(\"users.avro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc()\n",
    "validate_yaml(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "  \"type\" : \"record\",\n",
    "  \"namespace\" : \"com.test.avro\",\n",
    "  \"name\" : \"SystemModel\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"name\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"description\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"status\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"consumers\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"consumers\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"name\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"description\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"type\",\n",
    "        \"type\" : \"string\"\n",
    "      } ]\n",
    "    }\n",
    "  }, {\n",
    "    \"name\" : \"containers\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"containers\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"name\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"sysnonyms\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"description\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"technology\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"parentSystem\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"ciDataOwner\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"productOwner\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"applicationType\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"hostedAt\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"deploymentModel\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"personalData\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"confidentiality\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"mcv\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"maxSeverityLevel\",\n",
    "        \"type\" : \"long\"\n",
    "      }, {\n",
    "        \"name\" : \"sox\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"icfr\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"assignementGroup\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"operationalStatus\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"environments\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"relationships\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"record\",\n",
    "          \"name\" : \"relationships\",\n",
    "          \"fields\" : [ {\n",
    "            \"name\" : \"type\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"container\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"container\",\n",
    "              \"fields\" : [ {\n",
    "                \"name\" : \"name\",\n",
    "                \"type\" : \"string\"\n",
    "              } ]\n",
    "            }\n",
    "          } ]\n",
    "        }\n",
    "      }, {\n",
    "        \"name\" : \"components\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"record\",\n",
    "          \"name\" : \"components\",\n",
    "          \"fields\" : [ {\n",
    "            \"name\" : \"name\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"description\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"exposedAPIs\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"exposedAPIs\",\n",
    "              \"fields\" : [ { \"name\" : \"name\", \"type\" : \"string\" }, \n",
    "              { \"name\" : \"description\", \"type\" : \"string\" }, \n",
    "              { \"name\" : \"type\", \"type\" : \"string\" }, \n",
    "              { \"name\" : \"status\", \"type\" : \"string\" } ]\n",
    "            }\n",
    "          }, {\n",
    "            \"name\" : \"consumedAPIs\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"consumedAPIs\",\n",
    "              \"fields\" : [ {\n",
    "                \"name\" : \"name\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"description\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"status\",\n",
    "                \"type\" : \"string\"\n",
    "              } ]\n",
    "            }\n",
    "          } ]\n",
    "        }\n",
    "      } ]\n",
    "    }\n",
    "  } \n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = \"\"\"\n",
    "{\n",
    "  \"type\" : \"record\",\n",
    "  \"name\" : \"SystemModel\",\n",
    "  \"namespace\" : \"org.example.models.SystemModel\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"name\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"description\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"status\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"consumers\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"consumers\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"name\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"description\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"type\",\n",
    "        \"type\" : \"string\"\n",
    "      } ]\n",
    "    }\n",
    "  }, {\n",
    "    \"name\" : \"containers\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"containers\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"name\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"sysnonyms\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"description\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"technology\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"parentSystem\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"ciDataOwner\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"productOwner\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"applicationType\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"hostedAt\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"deploymentModel\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"personalData\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"confidentiality\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"mcv\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"maxSeverityLevel\",\n",
    "        \"type\" : \"long\"\n",
    "      }, {\n",
    "        \"name\" : \"sox\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"icfr\",\n",
    "        \"type\" : \"boolean\"\n",
    "      }, {\n",
    "        \"name\" : \"assignementGroup\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"operationalStatus\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"environments\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"relationships\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"record\",\n",
    "          \"name\" : \"relationships\",\n",
    "          \"fields\" : [ {\n",
    "            \"name\" : \"type\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"container\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"record\",\n",
    "              \"name\" : \"container\",\n",
    "              \"fields\" : [ {\n",
    "                \"name\" : \"name\",\n",
    "                \"type\" : \"string\"\n",
    "              } ]\n",
    "            }\n",
    "          } ]\n",
    "        }\n",
    "      }, {\n",
    "        \"name\" : \"components\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"record\",\n",
    "          \"name\" : \"components\",\n",
    "          \"fields\" : [ {\n",
    "            \"name\" : \"name\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"description\",\n",
    "            \"type\" : \"string\"\n",
    "          }, {\n",
    "            \"name\" : \"exposedAPIs\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"array\",\n",
    "              \"name\" : \"exposedAPIs\",\n",
    "              \"fields\" : [ {\n",
    "                \"name\" : \"name\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"description\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"type\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"status\",\n",
    "                \"type\" : \"string\"\n",
    "              } ]\n",
    "            }\n",
    "          }, {\n",
    "            \"name\" : \"consumedAPIs\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"array\",\n",
    "              \"name\" : \"consumedAPIs\",\n",
    "              \"fields\" : [ {\n",
    "                \"name\" : \"name\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"description\",\n",
    "                \"type\" : \"string\"\n",
    "              }, {\n",
    "                \"name\" : \"status\",\n",
    "                \"type\" : \"string\"\n",
    "              } ]\n",
    "            }\n",
    "          } ]\n",
    "        }\n",
    "      } ]\n",
    "    }\n",
    "  } ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_key_str = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"TestObject\",\n",
    "    \"namespace\": \"System-key\",\n",
    "    \"fields\": [{\n",
    "        \"name\": \"key\",\n",
    "        \"type\": \"string\"\n",
    "    }]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('avro_schema.avsc') as f:\n",
    "    schema_str = f.read()\n",
    "\n",
    "print(schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"topic5\"\n",
    "\n",
    "with open(\"avro_schema.avsc\") as f:\n",
    "    schema_str = f.read()\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient({'url': 'http://10.152.183.242:8081'})\n",
    "\n",
    "avro_serializer = AvroSerializer(schema_registry_client, schema_str)\n",
    "\n",
    "string_serializer = StringSerializer('utf_8')\n",
    "\n",
    "producer = Producer({'bootstrap.servers': '10.152.183.181:9094'})\n",
    "\n",
    "producer.produce(topic=topic, key=string_serializer('testkey', None), value=avro_serializer(data, SerializationContext(topic, MessageField.VALUE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient, Schema\n",
    "\n",
    "avro_schema = Schema(schema_str, 'AVRO')\n",
    "\n",
    "client = SchemaRegistryClient(\"http://10.152.183.242:8081\")\n",
    "\n",
    "schema_id = client.register_schema('test', avro_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.avro import AvroProducer\n",
    "\n",
    "producer = AvroProducer({'bootstrap.servers': '10.152.183.181:9094', 'schema.registry.url': 'http://10.152.183.242:8081'})\n",
    "\n",
    "producer.produce(topic=\"topic4\", value=data, value_schema=schema_str, key_schema=schema_key_str, key=\"testkey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(\n",
    "                             value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                             bootstrap_servers=\"10.152.183.181:9094\")\n",
    "producer.send('topic2', value=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka_schema_registry import prepare_producer\n",
    "\n",
    "producer = prepare_producer(bootstrap_servers=[\"10.152.183.181:9094\"], avro_schema_registry=\"http://10.152.183.242:8081\", topic_name=\"topic1\", value_schema=schema, num_partitions=1, replication_factor=1)\n",
    "\n",
    "producer.send(\"topic1\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('Python', ['tewst', 'py']), ('Java', ['java'])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Java'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import yaml\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_yaml(yaml_file: str) -> dict:\n",
    "    with open(yaml_file, mode='r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "def find_main_language(full_output = False):\n",
    "  languages = parse_yaml(\"languages.yml\")\n",
    "  print(languages.items())\n",
    "  matches = defaultdict(int)\n",
    "  for root, directory, filenames in os.walk(os.getcwd()):\n",
    "      for filename in filenames:\n",
    "        for key, value in languages.items():\n",
    "            for type in value:\n",
    "                if re.search(f\".({type}$)\", filename):\n",
    "                    size = os.path.getsize(root + '/' + filename)\n",
    "                    matches[key] += size\n",
    "  if(full_output):\n",
    "    return matches\n",
    "  else:\n",
    "    return max(matches, key=matches.get)\n",
    "str(find_main_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('name', 'poc-git-to-cmdb'), ('description', 'POC to send information about the app to Kafka')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [125], line 82\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m YAML_DATA[\u001b[39m'\u001b[39m\u001b[39mcontainers\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     78\u001b[0m         \u001b[39mprint\u001b[39m(item)\n\u001b[0;32m---> 82\u001b[0m filter_none(YAML_DATA)\n\u001b[1;32m     83\u001b[0m \u001b[39mprint\u001b[39m(YAML_DATA)\n",
      "Cell \u001b[0;32mIn [125], line 76\u001b[0m, in \u001b[0;36mfilter_none\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m     74\u001b[0m stack \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(d\u001b[39m.\u001b[39mitems())\n\u001b[1;32m     75\u001b[0m \u001b[39mprint\u001b[39m(stack[:\u001b[39m2\u001b[39m])\n\u001b[0;32m---> 76\u001b[0m check_values(stack[:\u001b[39m2\u001b[39;49m])\n\u001b[1;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m YAML_DATA[\u001b[39m'\u001b[39m\u001b[39mcontainers\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     78\u001b[0m     \u001b[39mprint\u001b[39m(item)\n",
      "Cell \u001b[0;32mIn [125], line 58\u001b[0m, in \u001b[0;36mcheck_values\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_values\u001b[39m(d):\n\u001b[0;32m---> 58\u001b[0m     stack \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(d\u001b[39m.\u001b[39;49mitems()) \n\u001b[1;32m     59\u001b[0m     visited \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m     60\u001b[0m     \u001b[39mwhile\u001b[39;00m stack: \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from contextlib import suppress\n",
    "\n",
    "YAML_DATA = {'name': 'poc-git-to-cmdb',\n",
    " 'description': 'POC to send information about the app to Kafka',\n",
    " 'containers': [{'name': 'poc-git-to-cmdb',\n",
    "   'synonyms': 'poc-git-to-kafka-cmdb-sync',\n",
    "   'description': 'POC',\n",
    "   'technology': None,\n",
    "   'team': None,\n",
    "   'applicationType': 'Tool',\n",
    "   'hostedAt': 'Azure Cloud',\n",
    "   'deploymentModel': 'On-Premise',\n",
    "   'containsPersonalData': False,\n",
    "   'confidentiality': 'Internal use',\n",
    "   'mcv': 'Not business critical',\n",
    "   'maxSeverityLevel': 4,\n",
    "   'containsFinancialData': False,\n",
    "   'assignementGroup': 'te',\n",
    "   'operationalStatus': 'Pipelined',\n",
    "   'environments': 'nl',\n",
    "   'components': {'name': 'Component name',\n",
    "    'description': 'what the system does',\n",
    "    'exposedAPIs': [{'name': 'Unique API name',\n",
    "      'description': 'What it can be used for',\n",
    "      'type': 'HTTP/JSON',\n",
    "      'status': 'TO_BE_IMPLEMENTED'},\n",
    "     {'name': 'Unique API name2',\n",
    "      'description': 'What it can be used for2',\n",
    "      'type': 'HTTP/JSON2',\n",
    "      'status': 'TO_BE_IMPLEMENTED2'}],\n",
    "    'consumedAPIs': [{'name': 'test100',\n",
    "      'description': 'What is it used for',\n",
    "      'status': 'TO_BE_IMPLEMENTED',\n",
    "      'read': True,\n",
    "      'write': True,\n",
    "      'execute': False},\n",
    "     {'name': 'test2000',\n",
    "      'description': 'What is it used for2',\n",
    "      'status': 'TO_BE_IMPLEMENTED2',\n",
    "      'read': True,\n",
    "      'write': False,\n",
    "      'execute': True}]}}]}\n",
    "\n",
    "def delete_keys_from_dict(d, to_delete):\n",
    "    if isinstance(to_delete, str):\n",
    "        to_delete = [to_delete]\n",
    "    if isinstance(d, dict):\n",
    "        for single_to_delete in set(to_delete):\n",
    "            if single_to_delete in d:\n",
    "                del d[single_to_delete]\n",
    "        for k, v in d.items():\n",
    "            delete_keys_from_dict(v, to_delete)\n",
    "    elif isinstance(d, list):\n",
    "        for i in d:\n",
    "            delete_keys_from_dict(i, to_delete)\n",
    "\n",
    "def check_values(d):\n",
    "    stack = list(d.items()) \n",
    "    visited = set()\n",
    "    while stack: \n",
    "        k, v = stack.pop()\n",
    "        if isinstance(v, dict):\n",
    "            print(k) \n",
    "            if k not in visited: \n",
    "                stack.extend(v.items()) \n",
    "        else: \n",
    "            if v == None or v == '':\n",
    "                print(\"%s: %s\" % (k, v)) \n",
    "                delete_keys_from_dict(d, k)\n",
    "        visited.add(k)\n",
    "\n",
    "def filter_none(d): \n",
    "    global YAML_DATA\n",
    "    stack = list(d.items())\n",
    "    print(stack[:2])\n",
    "    check_values(stack[:2])\n",
    "    for item in YAML_DATA['containers']:\n",
    "        print(item)\n",
    "    \n",
    "\n",
    "\n",
    "filter_none(YAML_DATA)\n",
    "print(YAML_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'poc-git-to-cmdb',\n",
       " 'description': 'POC to send information about the app to Kafka',\n",
       " 'containers': [{'name': 'poc-git-to-cmdb',\n",
       "   'synonyms': 'poc-git-to-kafka-cmdb-sync',\n",
       "   'description': 'POC',\n",
       "   'applicationType': 'Tool',\n",
       "   'hostedAt': 'Azure Cloud',\n",
       "   'deploymentModel': 'On-Premise',\n",
       "   'containsPersonalData': False,\n",
       "   'confidentiality': 'Internal use',\n",
       "   'mcv': 'Not business critical',\n",
       "   'maxSeverityLevel': 4,\n",
       "   'containsFinancialData': False,\n",
       "   'assignementGroup': 'te',\n",
       "   'operationalStatus': 'Pipelined',\n",
       "   'environments': 'nl',\n",
       "   'components': {'name': 'Component name',\n",
       "    'description': 'what the system does',\n",
       "    'exposedAPIs': [{'name': 'Unique API name',\n",
       "      'description': 'What it can be used for',\n",
       "      'type': 'HTTP/JSON',\n",
       "      'status': 'TO_BE_IMPLEMENTED'},\n",
       "     {'name': 'Unique API name2',\n",
       "      'description': 'What it can be used for2',\n",
       "      'type': 'HTTP/JSON2',\n",
       "      'status': 'TO_BE_IMPLEMENTED2'}],\n",
       "    'consumedAPIs': [{'name': 'test100',\n",
       "      'description': 'What is it used for',\n",
       "      'status': 'TO_BE_IMPLEMENTED',\n",
       "      'read': True,\n",
       "      'write': True,\n",
       "      'execute': False},\n",
       "     {'name': 'test2000',\n",
       "      'description': 'What is it used for2',\n",
       "      'status': 'TO_BE_IMPLEMENTED2',\n",
       "      'read': True,\n",
       "      'write': False,\n",
       "      'execute': True}]}}]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YAML_DATA = {'name': 'poc-git-to-cmdb',\n",
    " 'description': 'POC to send information about the app to Kafka',\n",
    " 'containers': [{'name': 'poc-git-to-cmdb',\n",
    "   'synonyms': 'poc-git-to-kafka-cmdb-sync',\n",
    "   'description': 'POC',\n",
    "   'technology': None,\n",
    "   'team': None,\n",
    "   'applicationType': 'Tool',\n",
    "   'hostedAt': 'Azure Cloud',\n",
    "   'deploymentModel': 'On-Premise',\n",
    "   'containsPersonalData': False,\n",
    "   'confidentiality': 'Internal use',\n",
    "   'mcv': 'Not business critical',\n",
    "   'maxSeverityLevel': 4,\n",
    "   'containsFinancialData': False,\n",
    "   'assignementGroup': 'te',\n",
    "   'operationalStatus': 'Pipelined',\n",
    "   'environments': 'nl',\n",
    "   'components': {'name': 'Component name',\n",
    "    'description': 'what the system does',\n",
    "    'exposedAPIs': [{'name': 'Unique API name',\n",
    "      'description': 'What it can be used for',\n",
    "      'type': 'HTTP/JSON',\n",
    "      'status': 'TO_BE_IMPLEMENTED'},\n",
    "     {'name': 'Unique API name2',\n",
    "      'description': 'What it can be used for2',\n",
    "      'type': 'HTTP/JSON2',\n",
    "      'status': 'TO_BE_IMPLEMENTED2'}],\n",
    "    'consumedAPIs': [{'name': 'test100',\n",
    "      'description': 'What is it used for',\n",
    "      'status': 'TO_BE_IMPLEMENTED',\n",
    "      'read': True,\n",
    "      'write': True,\n",
    "      'execute': False},\n",
    "     {'name': 'test2000',\n",
    "      'description': 'What is it used for2',\n",
    "      'status': 'TO_BE_IMPLEMENTED2',\n",
    "      'read': True,\n",
    "      'write': False,\n",
    "      'execute': True}]}}]}\n",
    "\n",
    "def remove_empties_from_dict(a_dict):\n",
    "    new_dict = {}\n",
    "    for k, v in a_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = remove_empties_from_dict(v)\n",
    "        if v is not None:\n",
    "            new_dict[k] = v\n",
    "    return new_dict or None\n",
    "\n",
    "def remove_none(obj):\n",
    "  if isinstance(obj, (list, tuple, set)):\n",
    "    return type(obj)(remove_none(x) for x in obj if x is not None or '')\n",
    "  elif isinstance(obj, dict):\n",
    "    return type(obj)((remove_none(k), remove_none(v))\n",
    "      for k, v in obj.items() if k is not None and v is not None or '')\n",
    "  else:\n",
    "    return obj\n",
    "\n",
    "remove_none(YAML_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownType",
     "evalue": "\n{\n  \"type\" : \"record\",\n  \"name\" : \"SystemModel\",\n  \"namespace\" : \"org.example.models.SystemModel\",\n  \"fields\" : [ {\n    \"name\" : \"name\",\n    \"type\" : \"string\"\n  }, {\n    \"name\" : \"description\",\n    \"type\" : \"string\"\n  }, {\n    \"name\" : \"status\",\n    \"type\" : \"string\"\n  }, {\n    \"name\" : \"consumers\",\n    \"type\" : {\n      \"type\" : \"record\",\n      \"name\" : \"consumers\",\n      \"fields\" : [ {\n        \"name\" : \"name\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"description\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"type\",\n        \"type\" : \"string\"\n      } ]\n    }\n  }, {\n    \"name\" : \"containers\",\n    \"type\" : {\n      \"type\" : \"record\",\n      \"name\" : \"containers\",\n      \"fields\" : [ {\n        \"name\" : \"name\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"sysnonyms\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"description\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"technology\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"parentSystem\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"ciDataOwner\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"productOwner\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"applicationType\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"hostedAt\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"deploymentModel\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"personalData\",\n        \"type\" : \"boolean\"\n      }, {\n        \"name\" : \"confidentiality\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"mcv\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"maxSeverityLevel\",\n        \"type\" : \"long\"\n      }, {\n        \"name\" : \"sox\",\n        \"type\" : \"boolean\"\n      }, {\n        \"name\" : \"icfr\",\n        \"type\" : \"boolean\"\n      }, {\n        \"name\" : \"assignementGroup\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"operationalStatus\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"environments\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"relationships\",\n        \"type\" : {\n          \"type\" : \"record\",\n          \"name\" : \"relationships\",\n          \"fields\" : [ {\n            \"name\" : \"type\",\n            \"type\" : \"string\"\n          }, {\n            \"name\" : \"container\",\n            \"type\" : {\n              \"type\" : \"record\",\n              \"name\" : \"container\",\n              \"fields\" : [ {\n                \"name\" : \"name\",\n                \"type\" : \"string\"\n              } ]\n            }\n          } ]\n        }\n      }, {\n        \"name\" : \"components\",\n        \"type\" : {\n          \"type\" : \"record\",\n          \"name\" : \"components\",\n          \"fields\" : [ {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n          }, {\n            \"name\" : \"description\",\n            \"type\" : \"string\"\n          }, {\n            \"name\" : \"exposedAPIs\",\n            \"type\" : {\n              \"type\" : \"array\",\n              \"name\" : \"exposedAPIs\",\n              \"fields\" : [ {\n                \"name\" : \"name\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"description\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"type\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"status\",\n                \"type\" : \"string\"\n              } ]\n            }\n          }, {\n            \"name\" : \"consumedAPIs\",\n            \"type\" : {\n              \"type\" : \"array\",\n              \"name\" : \"consumedAPIs\",\n              \"fields\" : [ {\n                \"name\" : \"name\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"description\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"status\",\n                \"type\" : \"string\"\n              } ]\n            }\n          } ]\n        }\n      } ]\n    }\n  } ]\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownType\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfastavro\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# with open('avro_schema.avsc') as f:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#       schema_str = f.read()\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m parsed_schema \u001b[39m=\u001b[39m fastavro\u001b[39m.\u001b[39;49mparse_schema(schema_str)\n",
      "File \u001b[0;32mfastavro/_schema.pyx:118\u001b[0m, in \u001b[0;36mfastavro._schema.parse_schema\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfastavro/_schema.pyx:141\u001b[0m, in \u001b[0;36mfastavro._schema._parse_schema\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnknownType\u001b[0m: \n{\n  \"type\" : \"record\",\n  \"name\" : \"SystemModel\",\n  \"namespace\" : \"org.example.models.SystemModel\",\n  \"fields\" : [ {\n    \"name\" : \"name\",\n    \"type\" : \"string\"\n  }, {\n    \"name\" : \"description\",\n    \"type\" : \"string\"\n  }, {\n    \"name\" : \"status\",\n    \"type\" : \"string\"\n  }, {\n    \"name\" : \"consumers\",\n    \"type\" : {\n      \"type\" : \"record\",\n      \"name\" : \"consumers\",\n      \"fields\" : [ {\n        \"name\" : \"name\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"description\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"type\",\n        \"type\" : \"string\"\n      } ]\n    }\n  }, {\n    \"name\" : \"containers\",\n    \"type\" : {\n      \"type\" : \"record\",\n      \"name\" : \"containers\",\n      \"fields\" : [ {\n        \"name\" : \"name\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"sysnonyms\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"description\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"technology\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"parentSystem\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"ciDataOwner\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"productOwner\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"applicationType\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"hostedAt\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"deploymentModel\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"personalData\",\n        \"type\" : \"boolean\"\n      }, {\n        \"name\" : \"confidentiality\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"mcv\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"maxSeverityLevel\",\n        \"type\" : \"long\"\n      }, {\n        \"name\" : \"sox\",\n        \"type\" : \"boolean\"\n      }, {\n        \"name\" : \"icfr\",\n        \"type\" : \"boolean\"\n      }, {\n        \"name\" : \"assignementGroup\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"operationalStatus\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"environments\",\n        \"type\" : \"string\"\n      }, {\n        \"name\" : \"relationships\",\n        \"type\" : {\n          \"type\" : \"record\",\n          \"name\" : \"relationships\",\n          \"fields\" : [ {\n            \"name\" : \"type\",\n            \"type\" : \"string\"\n          }, {\n            \"name\" : \"container\",\n            \"type\" : {\n              \"type\" : \"record\",\n              \"name\" : \"container\",\n              \"fields\" : [ {\n                \"name\" : \"name\",\n                \"type\" : \"string\"\n              } ]\n            }\n          } ]\n        }\n      }, {\n        \"name\" : \"components\",\n        \"type\" : {\n          \"type\" : \"record\",\n          \"name\" : \"components\",\n          \"fields\" : [ {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n          }, {\n            \"name\" : \"description\",\n            \"type\" : \"string\"\n          }, {\n            \"name\" : \"exposedAPIs\",\n            \"type\" : {\n              \"type\" : \"array\",\n              \"name\" : \"exposedAPIs\",\n              \"fields\" : [ {\n                \"name\" : \"name\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"description\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"type\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"status\",\n                \"type\" : \"string\"\n              } ]\n            }\n          }, {\n            \"name\" : \"consumedAPIs\",\n            \"type\" : {\n              \"type\" : \"array\",\n              \"name\" : \"consumedAPIs\",\n              \"fields\" : [ {\n                \"name\" : \"name\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"description\",\n                \"type\" : \"string\"\n              }, {\n                \"name\" : \"status\",\n                \"type\" : \"string\"\n              } ]\n            }\n          } ]\n        }\n      } ]\n    }\n  } ]\n}\n"
     ]
    }
   ],
   "source": [
    "import fastavro\n",
    "\n",
    "# with open('avro_schema.avsc') as f:\n",
    "#       schema_str = f.read()\n",
    "\n",
    "parsed_schema = fastavro.parse_schema(schema_str)\n",
    "# fastavro.validate(data, parsed_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "languagess = {}\n",
    "\n",
    "with open(\"languagestest.yml\", mode='r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "for key, value in data.items():\n",
    "        if 'extensions' in value:\n",
    "                extensions = []\n",
    "                for ext in value['extensions']:\n",
    "                        extensions.append(ext.replace(\".\",\"\"))\n",
    "                languagess[key] = extensions\n",
    "                        \n",
    "with open('languagess.yml', 'w') as outfile:\n",
    "        yaml.dump(languagess, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def parse_yaml() -> dict:\n",
    "    with open(\"test.yml\", mode='r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "data = parse_yaml()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'description', 'containers', 'name', 'sysnonyms', 'description', 'technology', 'ciDataOwner', 'productOwner', 'applicationType', 'hostedAt', 'deploymentModel', 'personalData', 'confidentiality', 'mcv', 'maxSeverityLevel', 'icfr', 'assignementGroup', 'operationalStatus', 'environments', 'components', 'components', 'name', 'description', 'exposedAPIs', 'name', 'description', 'type', 'status', 'consumedAPIs', 'name', 'description', 'status', 'read', 'write', 'execute']\n",
      "{'name': 'test', 'sysnonyms': 'poc-git-to-kafka-cmdb-sync', 'description': 'POC', 'technology': 'kafka', 'ciDataOwner': 'Aede van der Weij', 'productOwner': 'Thomas de Vries', 'applicationType': 'Tool', 'hostedAt': 'Azure Cloud', 'deploymentModel': 'On-Premise', 'personalData': False, 'confidentiality': 'Internal use', 'mcv': 'Not business critical', 'maxSeverityLevel': 4, 'icfr': False, 'assignementGroup': 'AMS_ITOnline_L3_Lists_and_Orders', 'operationalStatus': 'Pipelined', 'environments': 'nl', 'components': {'name': 'Component name', 'description': 'what the system does', 'exposedAPIs': [{'name': 'Unique API name', 'description': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}, {'name': 'Unique API name2', 'description': 'What it can be used for2', 'type': 'HTTP/JSON2', 'status': 'TO_BE_IMPLEMENTED2'}], 'consumedAPIs': [{'name': 'test1', 'description': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}, {'name': 'test2', 'description': 'What is it used for2', 'status': 'TO_BE_IMPLEMENTED2', 'read': True, 'write': False, 'execute': True}]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'poc-git-to-cmdb',\n",
       " 'description': 'POC to send information about the app to Kafka',\n",
       " 'containers': {'name': 'test',\n",
       "  'sysnonyms': 'poc-git-to-kafka-cmdb-sync',\n",
       "  'description': 'POC',\n",
       "  'technology': 'kafka',\n",
       "  'ciDataOwner': 'Aede van der Weij',\n",
       "  'productOwner': 'Thomas de Vries',\n",
       "  'applicationType': 'Tool',\n",
       "  'hostedAt': 'Azure Cloud',\n",
       "  'deploymentModel': 'On-Premise',\n",
       "  'personalData': False,\n",
       "  'confidentiality': 'Internal use',\n",
       "  'mcv': 'Not business critical',\n",
       "  'maxSeverityLevel': 4,\n",
       "  'icfr': False,\n",
       "  'assignementGroup': 'AMS_ITOnline_L3_Lists_and_Orders',\n",
       "  'operationalStatus': 'Pipelined',\n",
       "  'environments': 'nl',\n",
       "  'components': {'name': 'Component name',\n",
       "   'description': 'what the system does',\n",
       "   'exposedAPIs': [{'name': 'Unique API name',\n",
       "     'description': 'What it can be used for',\n",
       "     'type': 'HTTP/JSON',\n",
       "     'status': 'TO_BE_IMPLEMENTED'},\n",
       "    {'name': 'Unique API name2',\n",
       "     'description': 'What it can be used for2',\n",
       "     'type': 'HTTP/JSON2',\n",
       "     'status': 'TO_BE_IMPLEMENTED2'}],\n",
       "   'consumedAPIs': [{'name': 'test1',\n",
       "     'description': 'What is it used for',\n",
       "     'status': 'TO_BE_IMPLEMENTED',\n",
       "     'read': True,\n",
       "     'write': True,\n",
       "     'execute': False},\n",
       "    {'name': 'test2',\n",
       "     'description': 'What is it used for2',\n",
       "     'status': 'TO_BE_IMPLEMENTED2',\n",
       "     'read': True,\n",
       "     'write': False,\n",
       "     'execute': True}]}}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import re\n",
    "import copy\n",
    "from itertools import islice\n",
    "\n",
    "#Make recursice for dictionaries in the list of data\n",
    "\n",
    "def replace_key(data, keys, index = 0):\n",
    "    temp_data = {}\n",
    "    \n",
    "    for i, old_key in enumerate(data):\n",
    "        list_data = list(data.values())\n",
    "        temp_data[keys[i + index]] = list_data[i]\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "def translate_keys():\n",
    "    with open(\"test.yml\", mode='r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "\n",
    "    with open('avro_schema.avsc') as f:\n",
    "      schema_str = f.read()\n",
    "\n",
    "    schema_str = re.findall('(?<=\\\"name\"\\ : \")(.*?)(?=\\\")',schema_str)\n",
    "    del schema_str[0]\n",
    "    del schema_str[3]\n",
    "\n",
    "\n",
    "    print(schema_str)\n",
    "    first_data = replace_key(dict(islice(data.items(), 2)), schema_str)\n",
    "    second_data = replace_key(dict(islice(data.items(), 2, 3)), schema_str, 2)\n",
    "    third_data = replace_key(second_data[\"containers\"], schema_str, 3)\n",
    "    print(third_data)\n",
    "    fourth_data = replace_key(third_data[\"components\"], [\"name\", \"description\", \"exposedAPIs\", \"consumedAPIs\"])\n",
    "    fifth_data = list()\n",
    "\n",
    "    for value in fourth_data[\"exposedAPIs\"]:\n",
    "        fifth_data.append(replace_key(value, [\"name\", \"description\", \"type\", \"status\"]))\n",
    "    fourth_data[\"exposedAPIs\"] = fifth_data\n",
    "\n",
    "    sixth_data = list()\n",
    "\n",
    "    for value in fourth_data[\"consumedAPIs\"]:\n",
    "        sixth_data.append(replace_key(value, [\"name\", \"description\", \"status\", \"read\", \"write\", \"execute\"]))\n",
    "\n",
    "    fourth_data[\"consumedAPIs\"] = sixth_data\n",
    "\n",
    "    data = first_data \n",
    "    data[\"containers\"] = third_data\n",
    "    data[\"containers\"][\"components\"] = fourth_data\n",
    "\n",
    "    return data\n",
    "        \n",
    "\n",
    "translate_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'poc-git-to-cmdb',\n",
       " 'description': 'POC to send information about the app to Kafka',\n",
       " 'containers': [{'name': 'poc-git-to-cmdb',\n",
       "   'synonyms': 'poc-git-to-kafka-cmdb-sync',\n",
       "   'description': 'POC',\n",
       "   'technology': None,\n",
       "   'team': None,\n",
       "   'applicationType': 'Tool',\n",
       "   'hostedAt': 'Azure Cloud',\n",
       "   'deploymentModel': 'On-Premise',\n",
       "   'containsPersonalData': False,\n",
       "   'confidentiality': 'Internal use',\n",
       "   'mcv': 'Not business critical',\n",
       "   'maxSeverityLevel': 4,\n",
       "   'containsFinancialData': False,\n",
       "   'assignementGroup': 'te',\n",
       "   'operationalStatus': 'Pipelined',\n",
       "   'environments': 'nl',\n",
       "   'components': {'name': 'Component name',\n",
       "    'description': 'what the system does',\n",
       "    'exposedAPIs': [{'name': 'Unique API name',\n",
       "      'description': 'What it can be used for',\n",
       "      'type': 'HTTP/JSON',\n",
       "      'status': 'TO_BE_IMPLEMENTED'},\n",
       "     {'name': 'Unique API name2',\n",
       "      'description': 'What it can be used for2',\n",
       "      'type': 'HTTP/JSON2',\n",
       "      'status': 'TO_BE_IMPLEMENTED2'}],\n",
       "    'consumedAPIs': [{'name': 'test100',\n",
       "      'description': 'What is it used for',\n",
       "      'status': 'TO_BE_IMPLEMENTED',\n",
       "      'read': True,\n",
       "      'write': True,\n",
       "      'execute': False},\n",
       "     {'name': 'test2000',\n",
       "      'description': 'What is it used for2',\n",
       "      'status': 'TO_BE_IMPLEMENTED2',\n",
       "      'read': True,\n",
       "      'write': False,\n",
       "      'execute': True}]}}]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "from kafka import KafkaProducer\n",
    "from schema import Schema, SchemaError, Optional, Hook, Or\n",
    "from confluent_kafka import Producer, Consumer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from confluent_kafka import Producer, Consumer, DeserializingConsumer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField, StringDeserializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer, AvroDeserializer\n",
    "from confluent_kafka import Message\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "from itertools import islice\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "#Make recursice for dictionaries in the list of data\n",
    "\n",
    "def replace_key(data, keys, index = 0):\n",
    "    temp_data = {}\n",
    "\n",
    "    for i, old_key in enumerate(data):\n",
    "        list_data = list(data.values())\n",
    "        temp_data[keys[i + index]] = list_data[i]\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "def translate_keys(data):\n",
    "    with open('avro_schema.avsc') as f:\n",
    "      schema_str = f.read()\n",
    "\n",
    "    schema_str = re.findall('(?<=\\\"name\"\\ : \")(.*?)(?=\\\")',schema_str)\n",
    "    del schema_str[0]\n",
    "    del schema_str[3]\n",
    "\n",
    "    first_data = replace_key(dict(islice(data.items(), 2)), schema_str)\n",
    "    second_data = replace_key(dict(islice(data.items(), 2, 3)), schema_str, 2)\n",
    "    third_data = replace_key(second_data[\"containers\"], schema_str, 3)\n",
    "    fourth_data = replace_key(third_data[\"components\"], [\"name\", \"description\", \"exposedAPIs\", \"consumedAPIs\"])\n",
    "    fifth_data = list()\n",
    "\n",
    "    for value in fourth_data[\"exposedAPIs\"]:\n",
    "        fifth_data.append(replace_key(value, [\"name\", \"description\", \"type\", \"status\"]))\n",
    "    fourth_data[\"exposedAPIs\"] = fifth_data\n",
    "\n",
    "    sixth_data = list()\n",
    "\n",
    "    for value in fourth_data[\"consumedAPIs\"]:\n",
    "        sixth_data.append(replace_key(value, [\"name\", \"description\", \"status\", \"read\", \"write\", \"execute\"]))\n",
    "\n",
    "    fourth_data[\"consumedAPIs\"] = sixth_data\n",
    "\n",
    "    data = first_data \n",
    "    data[\"containers\"] = third_data\n",
    "    data[\"containers\"][\"components\"] = fourth_data\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_yaml() -> dict:\n",
    "    with open(\"test.yml\", mode='r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "# translate_keys(parse_yaml())\n",
    "# print(translate_keys())\n",
    "parse_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'devops.ah.it/support-assignment-group']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sname': 'poc-git-to-cmdb',\n",
       " 'sdescription': 'POC to send information about the app to Kafka',\n",
       " 'sstatus': 'pipelined',\n",
       " 'scontainers': {'sname': 'poc-git-tocmdb',\n",
       "  'ssysnonyms': 'poc-git-to-kafka-cmdb-sync',\n",
       "  'sdescription': 'POC',\n",
       "  'stechnology': None,\n",
       "  'sparentSystem': 'CMDB',\n",
       "  'sciDataOwner': 'Aede van der Weij',\n",
       "  'sproductOwner': 'Thomas de Vries',\n",
       "  'sapplicationType': 'Tool',\n",
       "  'shostedAt': 'Azure Cloud',\n",
       "  'sdeploymentModel': 'On-Premise',\n",
       "  'spersonalData': False,\n",
       "  'sconfidentiality': 'Internal use',\n",
       "  'smcv': 'Not business critical',\n",
       "  'smaxSeverityLevel': 4,\n",
       "  'ssox': None,\n",
       "  'sicfr': None,\n",
       "  'sassignementGroup': 'AMS_ITOnline_L3_Lists_and_Orders',\n",
       "  'soperationalStatus': 'Pipelined',\n",
       "  'senvironments': 'nl',\n",
       "  'scomponents': {'sname': 'Component name',\n",
       "   'sdescription': 'what the system does',\n",
       "   'sexposedAPIs': [{'sname': 'Unique API name',\n",
       "     'sdescription': 'What it can be used for',\n",
       "     'stype': 'HTTP/JSON',\n",
       "     'sstatus': 'TO_BE_IMPLEMENTED'},\n",
       "    {'sname': 'Unique API name2',\n",
       "     'sdescription': 'What it can be used for2',\n",
       "     'stype': 'HTTP/JSON2',\n",
       "     'sstatus': 'TO_BE_IMPLEMENTED2'}],\n",
       "   'sconsumedAPIs': [{'sname': 'test1',\n",
       "     'sdescription': 'What is it used for',\n",
       "     'sstatus': 'TO_BE_IMPLEMENTED',\n",
       "     'sread': True,\n",
       "     'swrite': True,\n",
       "     'sexecute': False},\n",
       "    {'sname': 'test1',\n",
       "     'sdescription': 'What is it used for2',\n",
       "     'sstatus': 'TO_BE_IMPLEMENTED2',\n",
       "     'sread': True,\n",
       "     'swrite': False,\n",
       "     'sexecute': True}]}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "def include_constructor(loader, node):\n",
    "  selector = loader.construct_sequence(node)\n",
    "  name = selector.pop(0)\n",
    "  print(selector)\n",
    "  with open(name) as f:\n",
    "    content = yaml.safe_load(f)\n",
    "  # walk over the selector items and descend into the loaded structure each time.\n",
    "  data = {}\n",
    "  for item in selector:\n",
    "    for key, value in content.items():\n",
    "      if key == item:\n",
    "        for name in selector:\n",
    "          content = content[name] \n",
    "        return content\n",
    "\n",
    "  return None\n",
    "\n",
    "yaml.add_constructor('!include', include_constructor, Loader=yaml.SafeLoader)\n",
    "\n",
    "with open(\"test.yml\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sname': 'poc-git-to-cmdb', 'sdescription': 'POC to send information about the app to Kafka', 'sstatus': 'pipelined', 'scontainers': {'sname': 'poc-git-tocmdb', 'ssysnonyms': 'poc-git-to-kafka-cmdb-sync', 'sdescription': 'POC', 'stechnology': None, 'sparentSystem': 'CMDB', 'sciDataOwner': 'Aede van der Weij', 'sproductOwner': 'Thomas de Vries', 'sapplicationType': 'Tool', 'shostedAt': 'Azure Cloud', 'sdeploymentModel': 'On-Premise', 'spersonalData': False, 'sconfidentiality': 'Internal use', 'smcv': 'Not business critical', 'smaxSeverityLevel': 4, 'ssox': None, 'sicfr': None, 'sassignementGroup': 'AMS_ITOnline_L3_SRE_Infra', 'soperationalStatus': 'Pipelined', 'senvironments': 'nl', 'scomponents': {'sname': 'Component name', 'sdescription': 'what the system does', 'sexposedAPIs': [{'sname': 'Unique API name', 'sdescription': 'What it can be used for', 'stype': 'HTTP/JSON', 'sstatus': 'TO_BE_IMPLEMENTED'}, {'sname': 'Unique API name2', 'sdescription': 'What it can be used for2', 'stype': 'HTTP/JSON2', 'sstatus': 'TO_BE_IMPLEMENTED2'}], 'sconsumedAPIs': [{'sname': 'test1', 'sdescription': 'What is it used for', 'sstatus': 'TO_BE_IMPLEMENTED', 'sread': True, 'swrite': True, 'sexecute': False}, {'sname': 'test1', 'sdescription': 'What is it used for2', 'sstatus': 'TO_BE_IMPLEMENTED2', 'sread': True, 'swrite': False, 'sexecute': True}]}}}\n"
     ]
    }
   ],
   "source": [
    "from Loader import Loader\n",
    "import yaml\n",
    "\n",
    "with open(\"test.yml\") as f:\n",
    "    data = yaml.load(f, Loader=Loader)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from confluent_kafka import Producer, Consumer, DeserializingConsumer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField, StringDeserializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer, AvroDeserializer\n",
    "from confluent_kafka import Message\n",
    "import time\n",
    "\n",
    "topic = [\"topic10\"]\n",
    "\n",
    "original = {'name': 'poc-git-to-cmdb', 'description': 'POC to send information about the app to Kafka', 'containers': {'name': 'poc-git-to-cmdb', 'sysnonyms': 'poc-git-to-kafka-cmdb-sync', 'description': 'POC', 'technology': 'kafka', 'ciDataOwner': 'Aede van der Weij', 'productOwner': 'Thomas de Vries', 'applicationType': 'Tool', 'hostedAt': 'Azure Cloud', 'deploymentModel': 'On-Premise', 'personalData': False, 'confidentiality': 'Internal use', 'mcv': 'Not business critical', 'maxSeverityLevel': 4, 'icfr': False, 'assignementGroup': 'AMS_ITOnline_L3_Lists_and_Orders', 'operationalStatus': 'Pipelined', 'environments': 'nl', 'components': {'name': 'Component name', 'description': 'what the system does', 'exposedAPIs': [{'name': 'Unique API name', 'description': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}, {'name': 'Unique API name2', 'description': 'What it can be used for2', 'type': 'HTTP/JSON2', 'status': 'TO_BE_IMPLEMENTED2'}], 'consumedAPIs': [{'name': 'test1', 'description': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}, {'name': 'test2', 'description': 'What is it used for2', 'status': 'TO_BE_IMPLEMENTED2', 'read': True, 'write': False, 'execute': True}]}}}\n",
    "\n",
    "def validate_names():\n",
    "\n",
    "    with open('avro_schema.avsc') as f:\n",
    "      schema_str = f.read()\n",
    "\n",
    "    schema_registry_client = SchemaRegistryClient({'url': 'http://10.152.183.242:8081'})\n",
    "\n",
    "    avro_deserializer = AvroDeserializer(schema_registry_client, schema_str)\n",
    "\n",
    "    string_deserializer = StringDeserializer('utf_8')\n",
    "\n",
    "\n",
    "    config = {'bootstrap.servers': '10.152.183.52:9094',\n",
    "    'group.id': 'aiufdsgdfjhdsagjhdsfjhfjdhajad;lkhkj',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'value.deserializer': avro_deserializer,\n",
    "    'key.deserializer': string_deserializer}\n",
    "    consumer = DeserializingConsumer(config)\n",
    "    try:\n",
    "        consumer.subscribe(topic)\n",
    "\n",
    "        timeout = time.time() + 60\n",
    "\n",
    "        while time.time() < timeout:\n",
    "            message = consumer.poll(timeout=1.0)\n",
    "            \n",
    "            if message is None: continue\n",
    "            if message.error():\n",
    "                print(message)\n",
    "            else:\n",
    "                if message.value() == original:\n",
    "                    print('same')\n",
    "    finally:\n",
    "        consumer.close()\n",
    "    return True\n",
    "\n",
    "validate_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "from kafka import KafkaProducer\n",
    "from schema import Schema, SchemaError, Optional, Hook, Or\n",
    "from confluent_kafka import Producer, Consumer, Avro\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from confluent_kafka import Producer, Consumer, DeserializingConsumer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField, StringDeserializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer, AvroDeserializer\n",
    "from confluent_kafka import Message\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "from itertools import islice\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "orginial_data = {'name': 'poc-git-to-cmdb', 'description': 'POC to send information about the app to Kafka', 'containers': {'name': 'poc-git-to-cmdb', 'sysnonyms': 'poc-git-to-kafka-cmdb-sync', 'description': 'POC', 'technology': 'kafka', 'ciDataOwner': 'Aede van der Weij', 'productOwner': 'Thomas de Vries', 'applicationType': 'Tool', 'hostedAt': 'Azure Cloud', 'deploymentModel': 'On-Premise', 'personalData': False, 'confidentiality': 'Internal use', 'mcv': 'Not business critical', 'maxSeverityLevel': 4, 'icfr': False, 'assignementGroup': 'AMS_ITOnline_L3_Lists_and_Orders', 'operationalStatus': 'Pipelined', 'environments': 'nl', 'components': {'name': 'Component name', 'description': 'what the system does', 'exposedAPIs': [{'name': 'Unique API name', 'description': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}, {'name': 'Unique API name2', 'description': 'What it can be used for2', 'type': 'HTTP/JSON2', 'status': 'TO_BE_IMPLEMENTED2'}], 'consumedAPIs': [{'name': 'test1', 'description': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}, {'name': 'test2', 'description': 'What is it used for2', 'status': 'TO_BE_IMPLEMENTED2', 'read': True, 'write': False, 'execute': True}]}}}\n",
    "\n",
    "def send_to_kafka(settings: dict, data: dict):\n",
    "    global YAML_DATA\n",
    "\n",
    "    topic = \"topic10\"\n",
    "\n",
    "    with open('avro_schema.avsc') as f:\n",
    "      schema_str = f.read()\n",
    "\n",
    "    schema_registry_client = SchemaRegistryClient({'url': 'http://10.152.183.242:8081'})\n",
    "\n",
    "    avro_serializer = AvroSerializer(schema_registry_client, schema_str)\n",
    "\n",
    "    string_serializer = StringSerializer('utf_8')\n",
    "\n",
    "    producer = Producer({'bootstrap.servers': '10.152.183.52:9094'})\n",
    "\n",
    "    producer.produce(topic=topic, key=string_serializer(data['name'], None), value=avro_serializer(data, SerializationContext(topic, MessageField.VALUE)))\n",
    "\n",
    "    producer.flush()\n",
    "\n",
    "for i in range(500):\n",
    "  orginial_data['name'] = orginial_data['name'] + str(i)\n",
    "  send_to_kafka({}, data=orginial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1013\n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import TopicPartition\n",
    "\n",
    "messages = []\n",
    "\n",
    "def validate_names():\n",
    "    with open('avro_schema.avsc') as f:\n",
    "      schema_str = f.read()\n",
    "\n",
    "    schema_registry_client = SchemaRegistryClient({'url': 'http://10.152.183.242:8081'})\n",
    "\n",
    "    avro_deserializer = AvroDeserializer(schema_registry_client, schema_str)\n",
    "\n",
    "    string_deserializer = StringDeserializer('utf_8')\n",
    "\n",
    "\n",
    "    config = {'bootstrap.servers': '10.152.183.52:9094',\n",
    "    'group.id': str(uuid.uuid4()),\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'value.deserializer': avro_deserializer,\n",
    "    'key.deserializer': string_deserializer}\n",
    "    consumer = DeserializingConsumer(config)\n",
    "    \n",
    "\n",
    "    topic_partition = TopicPartition(\"topic10\", partition=0)\n",
    "    low, high = consumer.get_watermark_offsets(topic_partition)\n",
    "    print(low)\n",
    "    print(high)\n",
    "    current_offset = 0\n",
    "\n",
    "    try:\n",
    "        consumer.subscribe([\"topic10\"])\n",
    "        \n",
    "        timeout = time.time() + 30\n",
    "\n",
    "        # log.info('Consuming data to see if data is already present')\n",
    "        while current_offset < high:\n",
    "            message = consumer.poll(timeout=1.0)\n",
    "            current_offset += 1\n",
    "            if message is None: continue\n",
    "            if message.error():\n",
    "                print(message)\n",
    "            else:\n",
    "                # log.info('Consumed data: %s', message.value())\n",
    "                messages.append(message.value())\n",
    "    finally:\n",
    "        consumer.close()\n",
    "    return True\n",
    "\n",
    "validate_names()\n",
    "\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_names():\n",
    "    with open('avro_schema.avsc') as f:\n",
    "      schema_str = f.read()\n",
    "\n",
    "    schema_registry_client = SchemaRegistryClient({'url': 'http://10.152.183.242:8081'})\n",
    "\n",
    "    avro_deserializer = AvroDeserializer(schema_registry_client, schema_str)\n",
    "\n",
    "    string_deserializer = StringDeserializer('utf_8')\n",
    "\n",
    "\n",
    "    config = {'bootstrap.servers': '10.152.183.52:9094',\n",
    "    'group.id': str(uuid.uuid4()),\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'value.deserializer': avro_deserializer,\n",
    "    'key.deserializer': string_deserializer}\n",
    "    consumer = DeserializingConsumer(config)\n",
    "\n",
    "    try:\n",
    "        consumer.subscribe([\"topic10\"])\n",
    "\n",
    "        topic_partition = TopicPartition(\"topic10\", partition=0)\n",
    "        low, high = consumer.get_watermark_offsets(topic_partition)\n",
    "        current_offset = 0\n",
    "\n",
    "        log.info('Consuming data to see if data is already present')\n",
    "        while current_offset < high:\n",
    "            message = consumer.poll(timeout=1.0)\n",
    "            current_offset += 1\n",
    "            if message is None: continue\n",
    "            if message.error():\n",
    "                print(message)\n",
    "            else:\n",
    "                # log.info('Consumed data: %s', message.value())\n",
    "                log.info(message.value() == YAML_DATA)\n",
    "                if message.value() == YAML_DATA:\n",
    "                    log.info('Data is already present and validated')\n",
    "                    exit(0)\n",
    "    finally:\n",
    "        consumer.close()\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'name': 'poc-git-to-cmdb', 'synonyms': 'poc-git-to-kafka-cmdb-sync', 'description': 'POC', 'technology': 'tech', 'team': 'team', 'productOwner': 'owner', 'applicationType': 'Tool', 'hostedAt': 'Azure Cloud', 'deploymentModel': 'On-Premise', 'dataConfidentiality': {'containsPersonalData': False, 'containsFinancialData': False, 'publiclyExposed': False, 'restrictedAccess': True}, 'missionCriticality': 'Not business critical', 'assignementGroup': 'te', 'operationalStatus': 'Pipelined', 'components': {'name': 'Component name', 'description': 'what the system does', 'exposedAPIs': [{'name': 'Unique API name', 'description': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}, {'name': 'Unique API name2', 'description': 'What it can be used for2', 'type': 'HTTP/JSON2', 'status': 'TO_BE_IMPLEMENTED2'}], 'consumedAPIs': [{'name': 'Unique API name', 'description': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}, {'name': 'Unique API name2', 'description': 'What is it used for2', 'status': 'TO_BE_IMPLEMENTED2', 'read': True, 'write': False, 'execute': True}]}}]\n",
      "\n",
      "{'containsPersonalData': False, 'containsFinancialData': False, 'publiclyExposed': False, 'restrictedAccess': True}\n",
      "\n",
      "{'name': 'Component name', 'description': 'what the system does', 'exposedAPIs': [{'name': 'Unique API name', 'description': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}, {'name': 'Unique API name2', 'description': 'What it can be used for2', 'type': 'HTTP/JSON2', 'status': 'TO_BE_IMPLEMENTED2'}], 'consumedAPIs': [{'name': 'Unique API name', 'description': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}, {'name': 'Unique API name2', 'description': 'What is it used for2', 'status': 'TO_BE_IMPLEMENTED2', 'read': True, 'write': False, 'execute': True}]}\n",
      "\n",
      "[{'name': 'Unique API name', 'description': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}, {'name': 'Unique API name2', 'description': 'What it can be used for2', 'type': 'HTTP/JSON2', 'status': 'TO_BE_IMPLEMENTED2'}]\n",
      "value\n",
      "[{'poc-git-to-cmdb': 'Unique API name', 'POC to send information about the app to Kafka': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}]\n",
      "\n",
      "[{'name': 'Unique API name', 'description': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}, {'name': 'Unique API name2', 'description': 'What is it used for2', 'status': 'TO_BE_IMPLEMENTED2', 'read': True, 'write': False, 'execute': True}]\n",
      "value\n",
      "[{'poc-git-to-cmdb': 'Unique API name', 'POC to send information about the app to Kafka': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}]\n",
      "value\n",
      "[{'poc-git-to-cmdb': 'poc-git-to-cmdb', 'synonyms': 'poc-git-to-kafka-cmdb-sync', 'POC to send information about the app to Kafka': 'POC', 'technology': 'tech', 'team': 'team', 'productOwner': 'owner', 'applicationType': 'Tool', 'hostedAt': 'Azure Cloud', 'deploymentModel': 'On-Premise', 'dataConfidentiality': {'containsPersonalData': False, 'containsFinancialData': False, 'publiclyExposed': False, 'restrictedAccess': True}, 'missionCriticality': 'Not business critical', 'assignementGroup': 'te', 'operationalStatus': 'Pipelined', 'components': {'poc-git-to-cmdb': 'Component name', 'POC to send information about the app to Kafka': 'what the system does', 'exposedAPIs': [{'poc-git-to-cmdb': 'Unique API name', 'POC to send information about the app to Kafka': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}], 'consumedAPIs': [{'poc-git-to-cmdb': 'Unique API name', 'POC to send information about the app to Kafka': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}]}}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [121], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m new_dict\n\u001b[1;32m     94\u001b[0m \u001b[39m# translate_keys(parse_yaml())\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m replace_keys(parse_yaml(),parse_yaml2())\n",
      "Cell \u001b[0;32mIn [121], line 89\u001b[0m, in \u001b[0;36mreplace_keys\u001b[0;34m(data_dict, key_dict)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[39mprint\u001b[39m()\n\u001b[1;32m     88\u001b[0m     \u001b[39mprint\u001b[39m(value)\n\u001b[0;32m---> 89\u001b[0m     new_dict[new_key] \u001b[39m=\u001b[39m replace_keys(value, key_dict)\n\u001b[1;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     new_dict[new_key] \u001b[39m=\u001b[39m value\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "def replace_key(data, keys, index = 0):\n",
    "    temp_data = {}\n",
    "    for i, old_key in enumerate(data):\n",
    "        list_data = list(data.values())\n",
    "        temp_data[keys[i + index]] = list_data[i]\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "def translate_keys(data):\n",
    "    with open('avro_schema.avsc') as f:\n",
    "      schema_str = f.read()\n",
    "\n",
    "    schema_str = re.findall('(?<=\\\"name\"\\ : \")(.*?)(?=\\\")',schema_str)\n",
    "    del schema_str[0]\n",
    "    del schema_str[3]\n",
    "    first_data = replace_key(dict(islice(data.items(), 2)), schema_str)\n",
    "    second_data = replace_key(dict(islice(data.items(), 2, 3)), schema_str, 2)\n",
    "    containers = list()\n",
    "    for container in second_data[\"containers\"]:\n",
    "\n",
    "        container_object = replace_key(container, schema_str, 3)\n",
    "        first_container = replace_key(dict(islice(container.items(), 0,10)), ['name', 'synonyms', 'description', 'technology', 'team', 'productOwner', 'applicationType', 'hostedAt', 'deploymentModel', 'dataConfidentiality'])\n",
    "        first_container['dataConfidentiality'] = replace_key(first_container['dataConfidentiality'], ['containsPersonalData','containsFinancialData','publiclyExposed','restrictedAccess'])\n",
    "        second_container = replace_key(dict(islice(container_object.items(), 10, 14)), ['missionCriticality', 'assignementGroup', 'operationalStatus', 'components'])\n",
    "\n",
    "        for key in first_container.keys():\n",
    "            container_object[key] = first_container[key]\n",
    "\n",
    "        for key in second_container.keys():\n",
    "            container_object[key] = second_container[key]\n",
    "        print(second_container)\n",
    "        print()\n",
    "        print(container_object)\n",
    "        \n",
    "        fourth_data = replace_key(container_object[\"components\"], [\"name\", \"description\", \"exposedAPIs\", \"consumedAPIs\"])\n",
    "        fifth_data = list()\n",
    "\n",
    "        for value in fourth_data[\"exposedAPIs\"]:\n",
    "            fifth_data.append(replace_key(value, [\"name\", \"description\", \"type\", \"status\"]))\n",
    "        fourth_data[\"exposedAPIs\"] = fifth_data\n",
    "\n",
    "        sixth_data = list()\n",
    "\n",
    "        for value in fourth_data[\"consumedAPIs\"]:\n",
    "            sixth_data.append(replace_key(value, [\"name\", \"description\", \"status\", \"read\", \"write\", \"execute\"]))\n",
    "\n",
    "        fourth_data[\"consumedAPIs\"] = sixth_data\n",
    "\n",
    "        data = first_data \n",
    "        containers.append(container_object)\n",
    "        \n",
    "    data[\"containers\"] = containers\n",
    "\n",
    "    return data\n",
    "\n",
    "import yaml\n",
    "\n",
    "def parse_yaml() -> dict:\n",
    "    with open(\"test.yml\", mode='r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "def parse_yaml2() -> dict:\n",
    "    with open(\"test2.yml\", mode='r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "import collections\n",
    "\n",
    "def replace_keys(data_dict, key_dict):\n",
    "    new_dict = {}\n",
    "    if isinstance(data_dict, list):\n",
    "        dict_value_list = list()\n",
    "        for inner_dict in data_dict:\n",
    "            dict_value_list.append(replace_keys(inner_dict, key_dict))\n",
    "            print('value')\n",
    "            print(dict_value_list)\n",
    "            return dict_value_list\n",
    "    else:\n",
    "        for key in data_dict.keys():\n",
    "            value = data_dict[key]\n",
    "            new_key = key_dict.get(key, key)\n",
    "            if isinstance(value, dict) or isinstance(value, list):\n",
    "                print()\n",
    "                print(value)\n",
    "                new_dict[new_key] = replace_keys(value, key_dict)\n",
    "            else:\n",
    "                new_dict[new_key] = value\n",
    "    return new_dict\n",
    "\n",
    "# translate_keys(parse_yaml())\n",
    "replace_keys(parse_yaml(),parse_yaml2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'team-sre-core'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def find_team():\n",
    "\n",
    "    path = Path(os.getcwd() + '/CODEOWNERS')\n",
    "    if not path.is_file():\n",
    "        log.error(\"CODEOWNERS file could not be found, please manually fill in team\")\n",
    "        exit(1)\n",
    "\n",
    "    with open(os.getcwd() + '/CODEOWNERS') as f:\n",
    "        code = f.read()\n",
    "\n",
    "    matches = re.findall(r\"(CODEOWNERS|\\*)[ \\t]+(@RoyalAholdDelhaize\\/)(.*)\", code)\n",
    "\n",
    "    return matches[0][-1]\n",
    "\n",
    "find_team()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 'containers' error:\n",
      "Or({'name': <class 'str'>, 'sysnonyms': <class 'str'>, 'description': <class 'str'>, Optional('technology'): <class 'str'>, Optional('team'): <class 'str'>, 'applicationType': Or('Business', 'Customer Facing', 'External Service', 'Infrastructure', 'Interface', 'Office', 'Tool', 'Unknown'), Optional('hostedAt'): Or('Amazon Web Services (AWS Cloud)', 'AT&T', 'Azure CF1', 'Azure CF2', 'Azure Cloud', 'DXC', 'Equinix', 'Google Cloud Platform', 'Hybric', 'Inlumi', 'Local server', 'Multi-Cloud', 'Not Applicable', 'Other', 'Salesforce', 'ServiceNow', 'Solvinity', 'Unit4', 'Unknown', 'User device', 'Azure'), 'deploymentModel': Or('BPO', 'CaaS', 'IaaS', 'On-Premise', 'PaaS', 'SaaS'), 'personalData': <class 'bool'>, 'confidentiality': <class 'str'>, 'mcv': Or('Highly business critical', 'Business critical', 'Not business critical', 'Not applicable'), 'maxSeverityLevel': Or(1, 2, 3, 4, 'Not applicable'), Optional('icfr'): <class 'bool'>, 'assignementGroup': <class 'str'>, 'operationalStatus': Or('Pipelined', 'Operational', 'Non-Operational', 'Submitted for decommissioning', 'Decommissioned', 'In decommissioning process'), 'environments': Or('nl', 'be'), 'components': {'name': <class 'str'>, 'description': <class 'str'>, 'exposedAPIs': [{'name': <class 'str'>, 'description': <class 'str'>, 'type': <class 'str'>, 'status': <class 'str'>}], 'consumedAPIs': [{'name': <class 'str'>, 'description': <class 'str'>, 'status': <class 'str'>, 'read': <class 'bool'>, 'write': <class 'bool'>, 'execute': <class 'bool'>}]}}) did not validate {'name': 'poc-git-to-cmdb', 'synonyms': 'poc-git-to-kafka-cmdb-sync', 'description': 'POC', 'applicationType': 'Tool', 'hostedAt': 'Azure Cloud', 'deploymentModel': 'On-Premise', 'containsPersonalData': False, 'confidentiality': 'Internal use', 'mcv': 'Not business critical', 'maxSeverityLevel': 4, 'containsFinancialData': False, 'assignementGroup': 'AMS_ITOnline_L3_Lists_and_Orders', 'operationalStatus': 'Pipelined', 'environments': 'nl', 'components': {'name': 'Component name', 'description': 'what the system does', 'exposedAPIs': [{'name': 'Unique API name', 'description': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}, {'name': 'Unique API name2', 'description': 'What it can be used for2', 'type': 'HTTP/JSON2', 'status': 'TO_BE_IMPLEMENTED2'}], 'consumedAPIs': [{'name': 'test100', 'description': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}, {'name': 'test2000', 'description': 'What is it used for2', 'status': 'TO_BE_IMPLEMENTED2', 'read': True, 'write': False, 'execute': True}]}}\n",
      "Missing keys: 'personalData', 'sysnonyms'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "from kafka import KafkaProducer\n",
    "from schema import Schema, SchemaError, Optional, Hook, Or\n",
    "from confluent_kafka import Producer, Consumer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from confluent_kafka import Producer, Consumer, DeserializingConsumer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField, StringDeserializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer, AvroDeserializer\n",
    "from confluent_kafka import Message\n",
    "from confluent_kafka import TopicPartition\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "global YAML_DATA\n",
    "\n",
    "def find_main_language(full_output = False):\n",
    "  languages = parse_yaml(\"/languages.yml\")\n",
    "  matches = defaultdict(int)\n",
    "  for root, directory, filenames in os.walk(os.getcwd()):\n",
    "      for filename in filenames:\n",
    "        for key, value in languages.items():\n",
    "            for type in value:\n",
    "                if re.search(f\"\\.({type}$)\", filename):\n",
    "                    size = os.path.getsize(root + '/' + filename)\n",
    "                    matches[key] += size\n",
    "  if(full_output):\n",
    "    return matches\n",
    "  else:\n",
    "    return max(matches, key=matches.get)\n",
    "\n",
    "def find_team():\n",
    "    log.info('finding team')\n",
    "    path = Path(os.getcwd() + '/CODEOWNERS')\n",
    "    if not path.is_file():\n",
    "        log.error(\"CODEOWNERS file could not be found, please manually fill in team\")\n",
    "        exit(1)\n",
    "\n",
    "    with open(os.getcwd() + '/CODEOWNERS') as f:\n",
    "        code = f.read()\n",
    "\n",
    "    matches = re.findall(r\"(CODEOWNERS|\\*)[ \\t]+(@RoyalAholdDelhaize\\/)(.*)\", code)\n",
    "\n",
    "    return matches[0][-1]\n",
    "\n",
    "def add_value(key):\n",
    "    log.info('add value ' + str(key))\n",
    "    global YAML_DATA\n",
    "    if(key == 'technology'):\n",
    "      YAML_DATA['containers'][0][key] = str(find_main_language())\n",
    "    elif(key == 'icfr'):\n",
    "      YAML_DATA['containers'][0][key] = False\n",
    "    elif(key == 'hostedAt'):\n",
    "      YAML_DATA['containers'][0][key] = \"Azure Cloud\"\n",
    "    elif(key == 'team'):\n",
    "      YAML_DATA['containers'][0][key] = find_team()\n",
    "\n",
    "schema_val = {\n",
    "    \"name\": str,\n",
    "    \"description\": str,\n",
    "\n",
    "    \"containers\": [{\n",
    "        \"name\": str,\n",
    "        \"sysnonyms\": str,\n",
    "        \"description\": str,\n",
    "        Optional(\"technology\", default= lambda : add_value('technology')): str,\n",
    "        Optional(\"team\", default= lambda : add_value('team')): str,\n",
    "        \"applicationType\": Or(\"Business\", \"Customer Facing\", \"External Service\", \"Infrastructure\", \"Interface\", \"Office\", \"Tool\", \"Unknown\"),\n",
    "        Optional(\"hostedAt\", default = lambda : add_value('hostedAt')): Or(\"Amazon Web Services (AWS Cloud)\", \"AT&T\", \"Azure CF1\", \"Azure CF2\", \"Azure Cloud\", \"DXC\", \"Equinix\", \"Google Cloud Platform\", \"Hybric\", \"Inlumi\", \"Local server\", \"Multi-Cloud\", \"Not Applicable\", \"Other\", \"Salesforce\", \"ServiceNow\", \"Solvinity\", \"Unit4\", \"Unknown\", \"User device\", \"Azure\"),\n",
    "        \"deploymentModel\": Or(\"BPO\", \"CaaS\", \"IaaS\", \"On-Premise\", \"PaaS\", \"SaaS\"),\n",
    "        \"personalData\": bool,\n",
    "        \"confidentiality\": str,\n",
    "        \"mcv\": Or(\"Highly business critical\", \"Business critical\", \"Not business critical\", \"Not applicable\"),\n",
    "        \"maxSeverityLevel\": Or(1,2,3,4, \"Not applicable\"),\n",
    "        Optional(\"icfr\", default= lambda : add_value('icfr')): bool,\n",
    "        \"assignementGroup\": str,\n",
    "        # operational = deployed to prod, pipelined = in development not yet released\n",
    "        \"operationalStatus\": Or(\"Pipelined\", \"Operational\", \"Non-Operational\", \"Submitted for decommissioning\", \"Decommissioned\", \"In decommissioning process\"),\n",
    "        \"environments\": Or(\"nl\", \"be\"),\n",
    "        \"components\": {\n",
    "            \"name\": str,\n",
    "            \"description\": str,\n",
    "            \"exposedAPIs\": [{\n",
    "                \"name\": str,\n",
    "                \"description\": str,\n",
    "                \"type\": str,\n",
    "                \"status\": str,\n",
    "            }],\n",
    "            \"consumedAPIs\": [{\n",
    "                \"name\": str,\n",
    "                \"description\": str,\n",
    "                \"status\": str,\n",
    "                \"read\": bool,\n",
    "                \"write\": bool,\n",
    "                \"execute\": bool,\n",
    "            }]\n",
    "        },\n",
    "    }]\n",
    "}\n",
    "\n",
    "def validate_yaml(yaml_data, verbose = False):\n",
    "    validator = Schema(schema_val)\n",
    "    try:\n",
    "        validator.validate(yaml_data)\n",
    "        if(verbose):\n",
    "          print('YML valid')\n",
    "        return True\n",
    "    except SchemaError as se:\n",
    "        if(verbose):\n",
    "          print(se)\n",
    "        return False\n",
    "\n",
    "YAML_DATA = {'name': 'poc-git-to-cmdb', 'description': 'POC to send information about the app to Kafka', 'containers': [{'name': 'poc-git-to-cmdb', 'synonyms': 'poc-git-to-kafka-cmdb-sync', 'description': 'POC', 'applicationType': 'Tool', 'hostedAt': 'Azure Cloud', 'deploymentModel': 'On-Premise', 'containsPersonalData': False, 'confidentiality': 'Internal use', 'mcv': 'Not business critical', 'maxSeverityLevel': 4, 'containsFinancialData': False, 'assignementGroup': 'AMS_ITOnline_L3_Lists_and_Orders', 'operationalStatus': 'Pipelined', 'environments': 'nl', 'components': {'name': 'Component name', 'description': 'what the system does', 'exposedAPIs': [{'name': 'Unique API name', 'description': 'What it can be used for', 'type': 'HTTP/JSON', 'status': 'TO_BE_IMPLEMENTED'}, {'name': 'Unique API name2', 'description': 'What it can be used for2', 'type': 'HTTP/JSON2', 'status': 'TO_BE_IMPLEMENTED2'}], 'consumedAPIs': [{'name': 'test100', 'description': 'What is it used for', 'status': 'TO_BE_IMPLEMENTED', 'read': True, 'write': True, 'execute': False}, {'name': 'test2000', 'description': 'What is it used for2', 'status': 'TO_BE_IMPLEMENTED2', 'read': True, 'write': False, 'execute': True}]}}]}\n",
    "\n",
    "validate_yaml(YAML_DATA, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import glob\n",
    "\n",
    "files = glob.glob(os.getcwd() + '/persons/*.yml')\n",
    "\n",
    "person_dict = {}\n",
    "\n",
    "def read_yaml_file(filename):\n",
    "    with open(filename, 'r') as stream:\n",
    "        try:\n",
    "            person = yaml.safe_load(stream)\n",
    "            name = person['person']['name']\n",
    "            name = name.replace(' ', '_')\n",
    "            info = {'teams': person['person']['teams'], 'roles': person['person']['roles']}\n",
    "            person_dict[name] = info\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "for file in files:\n",
    "    read_yaml_file(file)\n",
    "\n",
    "with open('persons.yml', 'w') as outfile:\n",
    "    yaml.safe_dump(person_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import glob\n",
    "\n",
    "files = glob.glob(os.getcwd() + '/teams/*.yml')\n",
    "\n",
    "team_dict = {}\n",
    "\n",
    "def read_yaml_file(filename):\n",
    "    with open(filename, 'r') as stream:\n",
    "        try:\n",
    "            team = yaml.safe_load(stream)\n",
    "            name = team['team']['name']\n",
    "            if 'area' in team['team'].keys():\n",
    "                info = {'area': team['team']['area'], 'description': team['team']['description']}\n",
    "            else:\n",
    "                info = {'description': team['team']['description']}\n",
    "            team_dict[name] = info\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "for file in files:\n",
    "    read_yaml_file(file)\n",
    "\n",
    "\n",
    "# print(team_dict)\n",
    "\n",
    "with open('teams.yml', 'w') as outfile:\n",
    "    yaml.safe_dump(team_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['team-sre-re', 'team-sre']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hessel_Bakker'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_product_owner(role):\n",
    "    global YAML_DATA\n",
    "    with open(\"persons.yml\", mode='r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    for k, v in data.items():\n",
    "        if v['teams'] is None:\n",
    "            continue\n",
    "        if 'team-sre' in v['teams']:\n",
    "            if role in v['roles']:\n",
    "                return k\n",
    "    return None\n",
    "\n",
    "find_product_owner()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
